CTRL FUTURE BOOTCAMP
=========================================
<table><tr>
<td> <img src="https://www.stendustri.com.tr/images/haberler/2022/02/koc_holding_ile_microsoft_tan_is_birligi_h116956_f396d.jpg" width="200""/> </td>
<td> <img src="https://global-uploads.webflow.com/6097e0eca1e875de53031ff6/6241a5ec363584013b7b1857_Patika%20logo%20(2).png" width="200""/> </td>
<td> <img src="https://cms.jibecdn.com/prod/ynvgroup-elev8/assets/OPENGRAPH-IMAGE-en-us-1635369201914.png" width="200""/> </td>    
</tr></table>



COVID-19 Data Solution for XYZ University
=========================================

*Enabling XYZ University to Make Informed Decisions Using COVID-19 Data in the Future*

Team 3\
20 September 2023

<img src="https://github.com/ebrusakar/ctrl-future-bootcamp/blob/main/pipeline_covid.png"  width="900" height="500">

Background
----------

At XYZ University, we understand the importance of data in addressing the challenges posed by the ongoing COVID-19 pandemic. As Data Engineers, our role is central to our efforts to create a robust data solution that helps us make better decisions and conduct in-depth research.

Business Objectives
-------------------

The primary business objective of this project is to develop a comprehensive COVID-19 data solution at XYZ University. This solution aims to provide timely, accurate, and actionable information to support data-driven decision-making, research initiatives, and community safety within the university. Specifically, our primary objectives are:

1.  Timely Tracking and Analysis: Build a platform that enables tracking, analysis, and visualization of COVID-19 data, allowing university stakeholders to monitor the pandemic's progress and impacts effectively.

In addition to the primary objective, there are several related business questions that XYZ University seeks to address through this project:

-   Empowering Decision-Makers: How can we empower university decision-makers, including administrators and safety personnel, with data-driven insights to guide operational and safety-related decisions?
-   Supporting Research Initiatives: How can we provide researchers at the university with access to a comprehensive and reliable COVID-19 dataset that supports in-depth research on the pandemic's various aspects?
-   Community Awareness: How can we ensure that the university community, including students, faculty, and staff, remains well-informed about the latest COVID-19 statistics and trends to make personal decisions and navigate campus life safely?
-   Data Accuracy: How can we ensure the accuracy and integrity of the COVID-19 data presented in the platform to build trust among users and stakeholders?

These objectives and related questions collectively drive the project's mission to harness the power of data in addressing the challenges posed by the ongoing COVID-19 pandemic within the university context. Please note that the project acknowledges that real-time data updates are not feasible due to data source limitations, and the platform will provide the most recent available data.

Business Success Criteria
-------------------------

The success of the COVID-19 Data Solution project at XYZ University will be evaluated based on several key criteria from a business point of view. These criteria are designed to assess the project's utility and impact in providing valuable insights and supporting data-driven decision-making:

### Effective Data Delivery:

-   Criterion: The project will be deemed successful if it can consistently deliver the most recent available COVID-19 data, considering data source limitations.
-   Measurement: Timeliness and data accuracy will be evaluated by comparing the platform's data with trusted external sources.

### User Engagement and Utilization:

-   Criterion: Success will be measured by the level of user engagement and utilization of the COVID-19 dashboard by university stakeholders, including administrators, researchers, and the wider university community.
-   Measurement: User engagement metrics, such as the number of dashboard visits, time spent on the platform, and the variety of reports accessed, will be tracked.

### Informed Decision-Making:

-   Criterion: The project's success will be determined by its ability to empower decision-makers with data-driven insights to guide operational and safety-related decisions.
-   Measurement: Feedback and surveys from decision-makers regarding the usefulness of the platform in informing their actions will be considered.

### Research Support and Accessibility:

-   Criterion: Success will be evaluated based on the platform's effectiveness in providing researchers with comprehensive and reliable COVID-19 datasets to support in-depth research on various pandemic aspects.
-   Measurement: The number and quality of research initiatives facilitated by access to the platform's data will be assessed.

### Community Awareness and Engagement:

-   Criterion: The project will be considered successful if it ensures that the university community remains well-informed about the latest COVID-19 statistics and trends, facilitating informed personal decisions and safe campus life navigation.
-   Measurement: Community awareness and satisfaction will be gauged through feedback and surveys on the platform's usefulness.

### Data Trust and Reliability:

-   Criterion: Success will be determined by the establishment of trust among users and stakeholders in the accuracy and reliability of the COVID-19 data presented in the platform.
-   Measurement: Data trust will be assessed through regular data quality assessments and user feedback.

### Sustainability and Adaptability:

-   Criterion: The project's success will depend on its sustainability and adaptability to evolving data needs, ensuring its usefulness beyond the immediate pandemic context.
-   Measurement: The platform's ability to accommodate changing requirements and future data challenges will be examined.

The evaluation of these criteria will involve a cross-functional team consisting of university administrators, researchers, data analysts, and active users of the platform within the university community. Their feedback, user surveys, and performance metrics will collectively determine whether the project has achieved its business objectives and delivered a successful and valuable outcome. It is essential to note that the project acknowledges the limitations of real-time data updates and focuses on providing the most recent available data.

Inventory of Resources
----------------------

### Personnel

| Name | Organization | Role | 
| --- | --- | --- |
| Emre Sağlam | XYZ University | Data Engineer | 
| Emre Apa | XYZ University | Data Engineer | 
| Elif Yeşim Koçoğlu | XYZ University | Data Engineer |
| Ebru Şakar | XYZ University | Data Engineer | 

### Data

| Location | Storage Type | Description | Contacts |
| --- | --- | --- | --- |
| Eurostat Website: Population Data | Online | Demographic data breaking down population by age groups. | Eurostat |
| Eurostat Website: Case & Deaths Data | Online | COVID-19 statistics including country-wise cases, deaths, daily counts, etc. | ECDC |
| ECDC Website: Country Response Data | Online | Information on countries' response measures to COVID-19. | ECDC |
| ECDC Website: Hospital Admission Data | Online | Data on hospital admissions, health indicators, and country-wise trends. | ECDC |
| ECDC Website: Testing Data | Online | COVID-19 testing information, including tests conducted, positivity rates. | ECDC |
| Our World in Data | Online | Comprehensive global COVID-19 statistics covering cases, testing, vaccination. | OWID |

These resources, including personnel, data sources, and access procedures, form the foundation of our COVID-19 data solution at XYZ University. We have established contacts with the respective organizations for data access, and regular updates ensure the reliability and relevance of the data. Additional assistance is available from Eurostat, ECDC, and Our World in Data for any data-related inquiries.

### Software

To complete the objectives of the COVID-19 Data Solution project at XYZ University, we will utilize a range of software tools and platforms that are well-suited for different aspects of data processing, analysis, and visualization. These software tools include:

-   Azure Data Factory
-   Azure Data Lake Storage Gen2
-   Azure Blob Storage
-   Azure Databricks
-   Azure SQL Database
-   Azure Machine Learning
-   Azure Data Studio
-   Microsoft SQL Server Management Studio
-   Microsoft Azure Storage Explorer
-   Power BI
-   Power BI Desktop

Requirements
------------

The successful execution of this project necessitates a clear set of requirements, encompassing various aspects such as data ingestion, transformation, and warehousing. These requirements ensure that the project aligns with its objectives and can deliver actionable insights to meet business needs.

### Data Ingestion Requirements

In this section, we delve into the specific requirements and processes governing the ingestion of data from various sources into our data ecosystem. Our project utilizes a combination of Blob Storage and HTTP connectors, leveraging the capabilities of Azure Data Factory to seamlessly transfer datasets into our Data Lake. We will explore the data ingestion processes for three distinct sources, each with its unique journey, to highlight our approach to ensuring data integrity and quality.

#### Population Data

During the transition of the Population dataset from Blob Storage to Data Lake, several essential processes were meticulously implemented to ensure data integrity and quality. These processes are detailed below:

1.  **Validation Activity**: We have implemented a Validation Activity in our pipeline, specifically designed to check the existence and attributes of the source data file. This activity employs a timeout mechanism to wait for the file to become available, and it monitors the file size to ensure completeness.

2.  **Get Metadata Activity**: We have integrated the Get Metadata Activity into our pipeline to retrieve metadata related to the source file. This information is crucial for subsequent validation and transformation steps.

3.  **Condition**: We have established a condition that compares the retrieved column count from the Get Metadata Activity with the expected count (e.g., 13 columns). If the condition is met (i.e., the column count matches the expectation), the pipeline proceeds to the next step.

4.  **Copy**: The Copy Activity is a fundamental part of our data ingestion process. It ensures that the validated and transformed data is transferred to the designated storage location for subsequent analysis and reporting.

5.  **Delete**: To maintain data cleanliness and avoid redundancy, we employ the Delete Activity to remove the source data file once it has been successfully copied to the destination storage. This step ensures that we do not process the same data multiple times.

#### ECDC Data

During the transition of the ECDC data from HTTP sources to Data Lake storage, we have meticulously executed a series of critical processes to maintain the integrity and quality of the dataset. These processes are detailed below:

1.  **Lookup**: The Lookup Activity serves as the initial step to retrieve a configuration file containing information about the files we need to process in the ECDC dataset. This step ensures that we have access to the necessary details for subsequent data transfer operations.

2.  **Foreach**: The Foreach Activity is employed to iterate through the records obtained from the configuration file. It allows us to process each file individually, enabling efficient data transfer operations for the entire dataset. We can execute these operations concurrently for increased efficiency.

3.  **Copy**: The Copy Activity is central to our data ingestion and preparation process. It is responsible for copying data from source URLs to our designated storage location, Azure Data Lake Gen 2. This step involves parameterization to handle various source URLs and sink file names, making it adaptable for different data sources and destinations.

#### Our World in Data

The transition of Our World in Data from HTTP source to Data Lake is achieved through a streamlined process called "copy activity." This method involves copying the COVID-19 dataset from the HTTP source to Data Lake, ensuring data integrity and accessibility in the new storage environment. It is a straightforward and effective way to facilitate this data migration.

### Data Transformation Requirements

This section outlines the specific requirements and methodologies employed to transform and integrate data from various sources into a structured and useful format for analysis and reporting. The transformation process encompasses a series of critical steps, including source transformation, filtering, selection, pivoting, lookup operations, and sink transformations. Below, we provide an overview of each transformation process, detailing how data is transformed to meet our project's objectives.

#### Cases and Deaths Source

We will use a series of transformations to clean, shape, and integrate the data from various sources into a structured and useful format. The transformation steps include source, filter, select, pivot, lookup, and sink transformations, which will be explained in detail below.

1.  **Source Transformation**: We begin by extracting data from multiple source files, each containing COVID-19 statistics for different regions and time periods. These source files are stored in Azure Data Lake Storage Gen2. The data is initially unstructured and requires transformation to be usable for analysis.

2.  **Filter Transformation**: In this step, we filter the extracted data to include only the records related to European countries. This filtering is based on specific criteria such as country codes or names that belong to the European region. Filtering helps us narrow down the dataset to our area of interest.

3.  **Select Transformation**: After filtering the data, we perform a select transformation. This step involves renaming columns, removing unnecessary columns (e.g., continent and rate 14-day columns), and changing data types if needed. We also renamed the date field to "report date" for clarity.

4.  **Pivot Transformation**: In this transformation, we pivot the daily count data to create separate columns for cases count and deaths count. The "indicator" column, which specifies whether the count relates to cases or deaths, is used as the pivot key. This process simplifies the data structure by consolidating counts into two distinct columns.

5.  **Lookup Transformation**: To enhance the dataset, we perform a lookup transformation. We extract a 2-digit country code from a lookup file stored in Azure Data Lake Storage Gen2. This lookup operation matches the country names in our data with their corresponding 2-digit country codes and adds this information to the dataset. This enrichment helps in data integration and analysis.

6.  **Sink Transformation**: The final step in our data transformation and integration process is the sink transformation. We write the cleaned and structured data into an output file, which is a delimited text file stored in Azure Data Lake Storage Gen2. This file contains the integrated and transformed data ready for further analysis and reporting.

#### Our World in Data

This dataset includes a wide array of COVID-19-related information collected from various countries and regions across the globe. It encompasses a rich set of metrics, such as total cases, total deaths, testing data, vaccination statistics, and more.

1. **Source Transformation:** Our initial step in the data transformation process for Our World in Data involved importing data from the raw data source. This dataset comprises comprehensive COVID-19-related information collected from diverse countries and regions.
2. **Select Transformation:** Moving on to the Select Transformation phase, we curated the dataset, creating a new one named SelectCasesDeaths. Within this dataset, we concentrated on essential COVID-19 metrics, ensuring that it is concise and pertinent for our analysis. The selected columns encompass 'date, location, total_cases, new_cases, total_deaths, new_deaths, icu_patients, hosp_patients, total_tests, new_tests, positive_rate, tests_units, total_vaccinations, new_vaccinations, people_vaccinated, and people_fully_vaccinated'.
3. **Sink Transformation:** In the final step of our data transformation process, we executed the Sink Transformation. During this phase, we exported the refined dataset to a designated destination. This clean and structured dataset, stripped of unnecessary information, will serve as the cornerstone for our subsequent data analysis and visualization efforts, supporting our objectives for Our World in Data.

#### Hospital Admission Source

The data we're working with contains valuable information related to hospital admissions during the COVID-19 pandemic. It includes details about countries, populations, dates, and counts of hospital and ICU admissions. However, this data arrives in various formats, making it challenging to work with directly. 

To make this data more manageable and insightful, we'll follow a series of data transformation steps. Each step simplifies and organizes the information, enhancing its usefulness. These steps include filtering out irrelevant data, selecting specific columns of interest, and merging data from weekly and daily sources.

Now, let's dive into these transformations, explaining each one in straightforward terms:

1. **Source Transformation:** In this project, our data source is the European Centre for Disease Prevention and Control (ECDC) COVID-19 dataset, which contains information on hospital admissions related to the pandemic. We begin by ingesting this data into our Azure Data Factory.
2. **Filter Transformation:** The first step in data transformation involves filtering the relevant data. We exclude records that do not pertain to hospital admissions, focusing on essential information such as country, population, and admission counts. Unnecessary columns and records are removed from the dataset.
3. **Select Transformation:** With the filtered data, we perform a "Select" transformation to choose specific columns for further processing. The selected columns include country, country code, population, reported date, hospital occupancy count, and ICU occupancy count. This step reduces the data to only the required fields.
4. **Pivot Transformation:** To make the data more structured, we utilize the "Pivot" transformation. Weekly and daily datasets have different date formats. We pivot the data to ensure uniformity, creating two separate streams for weekly and daily data. For weekly data, we use "Reported Year Week," and for daily data, we use "Reported Date" as key fields.
5. **Lookup Transformation:** Incorporating a "Lookup" transformation allows us to enhance our data with additional context. We connect our data with a calendar table ("DimDate") to include date-related information such as week start and end dates. This step facilitates more detailed analysis.
6. **Conditional Split Transformation:** We employ a "Conditional Split" transformation to split the data into two streams based on the type of dataset (weekly or daily). This branching enables us to apply specific transformations to each dataset independently.
7. **Derived Column Transformation:** Within each branch, we utilize "Derived Column" transformations to create calculated columns. For example, we calculate the "New Hospital Occupancy Count" and "New ICU Occupancy Count" to determine daily changes in hospital admissions.
8. **Aggregate Transformation:** For data summarization, we apply an "Aggregate" transformation. This allows us to group data by country and date, providing statistics such as the daily sum of hospital admissions.
9. **Join Transformation:** To consolidate our data streams, we perform a "Join" transformation. We combine the aggregated daily and weekly data to create a unified dataset, which includes both daily and weekly information.
10. **Sort Transformation:** For better data presentation, we utilize a "Sort" transformation. We arrange the data by "Reported Year Week" (descending order) and "Country" (ascending order) to display the most recent data at the top.
11. **Sink Transformation:** Finally, we use a "Sink" transformation to write the transformed data to Azure Data Lake Storage Gen2. This storage solution ensures that our data is organized in a structured manner, ready for further analysis or reporting.

#### Testing Source

In this phase, we leveraged HDInsight and Azure Data Factory to efficiently process and transform the Testing data. Specifically, we utilized a Hive script to perform these transformations. In this script, we first created databases to organize our data, including lookup tables for DimDate and DimCountry. These lookup tables helped us enrich our data with relevant information like date ranges and country codes. Then, within the Hive script, we joined our raw data with these lookup tables to create a processed testing dataset. This transformed data was stored in a designated container. Additionally, we configured Azure Data Factory to execute the Hive script on an HDInsight cluster, ensuring seamless integration of data transformation into our data workflow. This streamlined process ensures that our data is now well-structured and ready for further analysis and reporting.

#### Population Source

In this phase, we leveraged Azure Databricks and Azure Data Factory to process and integrate the population data efficiently. First, we mounted our Data Lake Storage in Databricks, creating a bridge between our data and Databricks. We then employed a Python notebook in Databricks to perform the data transformation. Specifically, we extracted valuable insights from the population data, focusing on the 2019 records for various countries. We transformed this raw data by splitting the age group and country code, making it more manageable for analysis. Additionally, we enriched our dataset by joining it with a lookup table, adding three essential columns: country name, three-digit country code, and population. Finally, we pivoted the data to convert rows of age group-specific information into columns, providing a structured and convenient format for future analysis. This process streamlined our population data, making it readily available for further analytics and reporting, a crucial step in our data pipeline.

### Data Warehousing Requirements

In this section of our project, we integrated and stored crucial datasets, including cases and deaths data, hospital admission data, our world in data, and population data. These datasets were transformed and prepared for reporting in our Azure SQL database. The process involved creating SQL tables within the database, ensuring that the structure of our tables matched the data we wanted to ingest.

The following SQL scripts give examples of creating tables:

```sql
CREATE SCHEMA covid_reporting
GO
CREATE TABLE covid_reporting.cases_and_deaths
(
    country VARCHAR(100),
    country_code_2_digit VARCHAR(2),
    country_code_3_digi VARCHAR(3),
    population BIGINT,
    cases_count BIGINT,
    deaths_count BIGINT,
    reported_date DATE,
    source VARCHAR(500)
)
GO

CREATE TABLE covid_reporting.hospital_admissions_daily
(
    country VARCHAR(100),
    country_code_2_digit VARCHAR(2),
    country_code_3_digit VARCHAR(3),
    population BIGINT,
    reported_date DATE,
    hospital_occupancy_count BIGINT,
    icu_occupancy_count BIGINT,
    source VARCHAR(500)
)
GO

CREATE TABLE covid_reporting.testing
(
    country VARCHAR(100),
    country_code_2_digit VARCHAR(2),
    country_code_3_digit VARCHAR(3),
    year_week VARCHAR(8),
    week_start_date DATE,
    week_end_date DATE,
    new_cases BIGINT,
    tests_done BIGINT,
    population BIGINT,
    testing_data_source VARCHAR(500)
)
GO

CREATE TABLE covid_reporting.owid_cases_and_deaths
(
    date DATE,
    total_cases BIGINT,
    new_cases BIGINT,
    total_deaths BIGINT,
    new_deaths BIGINT,
    icu_patients BIGINT,
    hosp_patients BIGINT,
    total_tests BIGINT,
    new_tests BIGINT,
    positive_rate FLOAT,
    tests_units VARCHAR(100),
    total_vaccinations BIGINT,
    new_vaccinations BIGINT,
    people_vaccinated BIGINT,
    people_fully_vaccinated BIGINT
)
GO

-- Some other scripts are also used to satisfy our frequently used data needs. For instance, having the population distributed by age groups, we frequently needed the number of the total populations by countries. We added a population column by the following SQL script:

ALTER TABLE covid_reporting.population
ADD population BIGINT

UPDATE covid_reporting.population
SET population = [age(0-4)] + [age(5-14)] + [age(15-24)] + [age(25-64)] + [age(65+)]

-- We used Data Factory's Copy Data Activity to efficiently copy the processed data from our Data Lake into the SQL tables. Special attention was given to mapping and data cleansing, as we aimed to maintain data integrity and consistency. This organized and centralized repository of data in our Azure SQL database now serves as the foundation for our reporting and analysis tasks.
```

### Batch Analysis and Historical Insights

This section focuses on developing deep-analysis methods utilizing stored raw data. In previous steps, we have already stored historical raw data in our Azure Data Lake Storage Gen 2. The goal can be achieved using many different Azure resources such as Azure Databricks, Azure Synapse Analytics, and Azure Machine Learning platform. Due to the limitations of the Student account and considering the size of the raw data that we will work on, we shifted our scope to the machine learning platform.

As mentioned earlier we have raw Our World in Data data stored in our Azure Data Lake Storage Gen 2. As our first step, we have created a data asset in Azure Machine Learning Studio. Next, we continued with creating a Jupyter Notebook and a compute instance. 4 cores CPU, 14GB RAM, and 28 GB disk is more than enough for our purposes.

Running the following code we have created a pandas data frame that stores the raw data:

```python
# azureml-core of version 1.0.72 or higher is required
# azureml-dataprep[pandas] of version 1.1.34 or higher is required
from azureml.core import Workspace, Dataset
subscription_id = '************************'
resource_group = '************************'
workspace_name = '************************'
workspace = Workspace(subscription_id, resource_group, workspace_name)
dataset = Dataset.get_by_name(workspace, name='owid-covid-datalake')
df = dataset.to_pandas_dataframe()
```

Then, data is transformed into a pyspark data frame and data is explored. Using the following script we get some insights into how the mortality rate changed given a time interval of 2021-01-01 to 2021-02-28:

```python
for i, this_day in enumerate(dates_list):
    this_day_top_10 = df.filter(F.col('date') == this_day).orderBy("total_cases_per_million", ascending=False).select(["location","total_cases_per_million"]).take(10)
    if i == 0:
        ct_list = [(this_day_top_10[x][0],this_day_top_10[x][1]) for x in range(10)]
        print("During "+this_day+", the top 10 countries with the highest number of total cases per million were:")
        for country, instance in ct_list:
            print(f"▶ {country}, with {instance} total cases per million.")
        new_set = set(ct_list[x][0] for x in range(10))
    elif i == len(dates_list)-1:
        ct_list = [(this_day_top_10[x][0],this_day_top_10[x][1]) for x in range(10)]
        print("During "+this_day+", the top 10 countries with the highest mortality rate were:")
        for country, instance in ct_list:
            print(f"▶ {country}, with mortality rate {instance}%.")
    else:
        new_set = set(this_day_top_10[x][0] for x in range(10))
        if new_set != old_set:
            left_out = old_set-new_set
            new_additions = new_set-old_set
            print("This was the top ten until "+this_day+", when "+", ".join(str(s) for s in new_additions)+" joined the list, replacing "+", ".join(str(s) for s in left_out)+".")
    new_set, old_set = set(), new_set
```

An important part of the output of the above code is: During 2021-01-01, the top 10 countries with the highest mortality rate were:

-   Peru, with mortality rate 0.27%.
-   San Marino, with mortality rate 0.18%.
-   Belgium, with mortality rate 0.17%.
-   United Kingdom, with mortality rate 0.14%.
-   North Macedonia, with mortality rate 0.14%.
-   Slovenia, with mortality rate 0.14%.
-   Bosnia and Herzegovina, with mortality rate 0.13%.
-   Italy, with mortality rate 0.13%.
-   Mexico, with mortality rate 0.12%.
-   Czechia, with mortality rate 0.11%.

During 2021-02-28, the top 10 countries with the highest mortality rate were:

-   Peru, with mortality rate 0.36%.
-   Gibraltar, with mortality rate 0.28%.
-   San Marino, with mortality rate 0.22%.
-   United Kingdom, with mortality rate 0.22%.
-   Czechia, with mortality rate 0.20%.
-   Slovenia, with mortality rate 0.20%.
-   Belgium, with mortality rate 0.19%.
-   North Macedonia, with mortality rate 0.17%.
-   Mexico, with mortality rate 0.17%.
-   Italy, with mortality rate 0.17%.

As shown here, with the power of the Azure Machine Learning notebooks, we developed insights on raw data within minutes.

Assumptions
------------

In addition to the project requirements, it's crucial to acknowledge certain assumptions that underlie the XYZ University COVID-19 Data Solution project. These assumptions provide context for the project's underlying beliefs, which may impact its validity. Understanding these assumptions is essential for addressing potential uncertainties and ensuring the project's success. Here, we detail the key assumptions made in the project.

1. **Data Accuracy:** We assume that data from trusted sources is accurate. We'll continuously validate it by cross-referencing multiple sources.

2. **Data Availability:** Our assumption is that data sources will provide timely updates.

3. **Data Transformation:** We assume that the transformation processes outlined in the project plan are correct. We'll rigorously test and validate these processes.

4. **User Requirements:** Our Power BI dashboard should meet user needs. We'll validate this through user feedback and usability testing.

5. **Data Privacy:** We're assuming that we have the right privacy measures in place for sensitive data. Regular audits and compliance checks will verify this.

6. **Data Security:** We assume our security measures effectively protect our data. Periodic security assessments and penetration testing will verify their effectiveness.

7. **Legal Compliance:** We assume that our data usage agreements are legally sound. We'll undergo legal reviews and compliance checks to confirm.

8. **Project Resources:** We're assuming we have sufficient resources available, including personnel and hardware. We'll continuously monitor and allocate resources as needed.

9. **Data Governance:** We assume our data governance policies are being followed. Regular audits and compliance checks will ensure adherence.

10. **Data Transparency:** Transparency in data sources and methodology is important. We'll maintain documentation and promote transparency initiatives throughout the project.

Constraints
------------------------

**Limited Resource Quotas:** Azure student accounts often come with limited resource quotas, including storage, compute, and bandwidth. These limitations may impact the project's ability to handle large volumes of data efficiently.

**Budgetary Constraints:** Azure student accounts are typically subject to budget restrictions. Project costs, such as data storage and compute resources, must remain within these constraints to avoid unexpected overages.

**Data Processing Speed:** Due to resource limitations, data processing speed in Azure student accounts may not match the project's ideal processing requirements, potentially causing delays in data transformations.

**Scalability Challenges:** Scalability may be constrained as Azure student accounts may not allow easy scaling up of resources to accommodate growing data volumes or increased user demand.

**Access to Premium Services:** Some premium Azure services and features may not be available with student accounts, limiting access to advanced tools that could enhance project capabilities.

**Support Limitations:** Student accounts may have limited access to Azure support, potentially affecting the project's ability to quickly resolve technical issues.

**Resource Sharing:** Since student accounts may be shared among multiple users, resource contention and allocation challenges may arise, impacting project performance.

**Data Backup and Retention:** Student accounts may have limitations on data backup and retention policies, potentially affecting data reliability and recovery options.

**Data Security and Compliance:** Ensuring data security and compliance with regulations may be more challenging due to the constraints of student accounts, potentially leading to data privacy risks.

**Project Resource Allocation:** Resource allocation for the project, including storage, compute, and data handling tools, must be carefully managed within the constraints of Azure student accounts to prevent resource shortages.

These constraints highlight the need for efficient resource management and budgetary planning within the limitations of Azure student accounts while executing the XYZ University COVID-19 Data Solution project.

Risks and Contingencies
------------------------

**Azure Student Account Budget Limitation:**
- **Risk:** The Azure student account comes with a budget limit of $100, which may be exceeded during the project.
- **Contingency:** To address this risk, a free account has been initiated to ensure that budget constraints do not hinder the project's progress.

**Data Shortage Risk:**
- **Risk:** There is a risk of data shortage or unavailability from the selected sources, which could disrupt the project's data processing.
- **Contingency:** To mitigate this risk, the project has proactively utilized multiple diverse data sources to ensure data availability and continuity.

**Team Cohesion and Role Distribution:**
- **Risk:** To maintain team cohesion and prevent disruptions, it is essential to effectively distribute roles and responsibilities among team members.
- **Contingency:** To address this risk, role distribution has been organized through a shared account, ensuring smooth collaboration and project progress.

Terminology
------------------------

- **Dashboard:** A visual representation of data that provides at-a-glance insights into key metrics, facilitating data-driven decision-making.

- **Data Ingestion:** The process of collecting and importing data from various sources into a storage or processing system for analysis.

- **Data Preparation:** The steps taken to clean, transform, and structure raw data into a format suitable for analysis.

- **Data Transformation:** The process of converting data from one format or structure into another to facilitate analysis or reporting.

- **Data Integration:** Combining data from different sources into a unified view, enabling comprehensive analysis.

- **Data Lake:** A centralized repository for storing vast amounts of raw data in its native format until it is needed for analysis.

- **Data Warehouse:** A structured and optimized database designed for efficient querying and reporting.

Costs and Benefits
------------------

**Azure Data Factory v2:**
- **Cost:** $28.93
- **Description:** Expenses for Azure Data Factory v2.
- **Benefits:** Improved data integration and ETL processes for efficient data handling.

**SQL Database:** 
- **Cost:** $13.33
- **Description:** Costs related to maintaining SQL databases.
- **Benefits:** Reliable data warehousing to support data analysis and reporting.

**Virtual Machines:**
- **Cost:** $2.17
- **Description:** Azure Machine Learning - Compute Instance(D3 v2/DS3 v2)
- **Benefits:** Compute power to run Jupyter notebooks.

**Storage:**
- **Cost:** $0.43
- **Description:** Disk for Virtual Machine(P10 LRS Disk)
- **Benefits:** Makes it possible to use compute instance on Azure Machine Learning Platform.

**Azure Databricks:**
- **Cost:** $0.24
- **Description:** Expenses for Azure Databricks.
- **Benefits:** Enhanced data processing and transformation capabilities.

**Storage:**
- **Cost:** $0.11
- **Description:** Costs for storing project-related data.
- **Benefits:** Secure data storage for project files and datasets.

**Bandwidth:**
- **Cost:** <$0.01
- **Description:** Minimal expenses related to data transfer.
- **Benefits:** Efficient data transfer and minimal latency.

**Event Grid:**
- **Cost:** $0.00
- **Description:** Negligible costs for using Azure Event Grid.
- **Benefits:** Event-driven processes for automation and data updates.

**Total Project Costs:** $44.79

Project Goals
--------------

In alignment with the overarching business objective of enhancing COVID-19 data management at XYZ University, our technical goals encompass the development of a comprehensive COVID-19 Data Solution. The primary objective is to create a user-friendly, accessible COVID-19 Dashboard tailored for the XYZ University community, researchers, and decision-makers. This dashboard will enable users to effectively track and analyze COVID-19 trends, including cases, deaths, hospital admissions, and testing statistics, across various countries and regions. It will feature customization options for date ranges and specific countries, providing valuable insights.

While the ideal scenario would involve real-time data updates, we acknowledge the challenge of data source inconsistencies and delays, making it more practical to provide near-real-time or periodically updated data to ensure data reliability and accuracy. Our technical goals also encompass robust data ingestion, transformation, integration, and warehousing processes to ensure data quality and accessibility. Through these technical pursuits, we aim to empower informed decision-making, support research initiatives, and foster data-driven insights within XYZ University's COVID-19 response efforts.

Project Outputs
---------------

The outputs of the COVID-19 Data Solution project are designed to enable the achievement of the business objectives and provide valuable insights to university stakeholders. These outputs encompass various data components, pipelines, and reports:

1. **Integrated COVID-19 Data:** The project outputs include integrated COVID-19 data from diverse sources, which will be consistently updated and maintained. This data comprises daily records of new deaths, new cases, new tests, age 65+ population ratios, and hospital-related statistics.

2. **Data Pipelines:** To ensure the continuous flow of data, the project implements data pipelines that automate the ingestion, transformation, and delivery of COVID-19 data into the university's data platform. These pipelines are designed to handle data from multiple sources efficiently.

3. **Power BI Dashboard:** The centerpiece of the project outputs is the Power BI dashboard, which provides an intuitive and interactive interface for university stakeholders to access and analyze COVID-19 data. The dashboard features the following components:
   - Sum of New Deaths: A visual representation of the total number of new deaths attributed to COVID-19, with options to filter data by date interval and location.
   - Sum of New Cases: A visual representation of the total number of new COVID-19 cases, allowing users to customize date intervals and geographic locations.
   - Sum of New Cases by Location: A map view displaying the distribution of new cases across different geographic locations, adjustable by date interval.
   - Sum of New Tests by Location: A map visualization showing the distribution of new COVID-19 tests performed in various locations, with date interval flexibility.
   - Age 65+ Divided by Population: A graphical representation of the ratio of individuals aged 65 and above to the total population in specific regions, offering insights into vulnerable demographics.
   - Sum of New Deaths by Location: A map view depicting the geographic distribution of new deaths due to COVID-19, with customizable date intervals and location filters.
   - Sum of New Cases by Location: A map visualization highlighting the spatial distribution of new COVID-19 cases by location, with options to select date intervals.
   - Sum of Hospital Patients by Date: A time-series chart illustrating the total number of hospital patients over time, allowing users to analyze trends and patterns.
   - Sum of ICU Patients by Date: A time-series chart displaying the total number of ICU patients over time, aiding in the assessment of critical care capacity.
   - Interactive Filters: The Power BI dashboard offers interactive filters that enable users to select specific date intervals and geographic locations, providing a tailored view of COVID-19 data.

4. **Reports:** In addition to the interactive dashboard, the project generates regular reports summarizing key COVID-19 statistics and insights. These reports are designed for university administrators, researchers, and decision-makers to stay informed about the pandemic's impact on campus.

These project outputs collectively empower university stakeholders to gain timely and actionable insights from COVID-19 data, supporting informed decision-making, research initiatives, and community engagement. The interactive dashboard and reports provide a comprehensive view of the pandemic's dynamics while ensuring data accessibility and usability.

Project Success Criteria
------------------------

The success of the COVID-19 Data Solution project at XYZ University will be evaluated based on the following criteria:

1. **Timely Data Integration:** The project will be considered successful if it integrates COVID-19 data from multiple sources into the university's data platform within specified timeframes, meeting predefined data ingestion schedules.

2. **Data Quality and Accuracy:** Success will be determined by the project's ability to maintain high data quality and accuracy throughout the data preparation process, assessed through periodic data quality assessments and validation against external sources.

3. **Platform Usability:** The project's success will be contingent on the ease of use and accessibility of the COVID-19 data platform for university stakeholders, based on user feedback and usability testing results indicating the platform's ease of navigation and usefulness.

4. **Data-Driven Insights:** Success will be evaluated based on the extent to which the platform provides actionable insights and supports data-driven decision-making, with feedback and surveys from users highlighting instances where data insights influenced their decisions.

5. **Research Facilitation:** The project's success will be assessed by its effectiveness in facilitating COVID-19 research initiatives within the university, measured by the number and quality of research projects enabled by access to the platform's data.

6. **Community Engagement:** Success will be measured by the level of engagement and interaction of the university community with the COVID-19 data platform, including metrics related to platform visits, interactions, and feedback.

7. **Platform Reliability:** The project's success will depend on the reliability and uptime of the COVID-19 data platform, with measurement based on platform uptime and performance metrics indicating its availability for users.

8. **Adaptability and Scalability:** Success will be determined by the platform's ability to adapt to changing data needs and scale to accommodate future requirements, documented through platform enhancements and scalability measures implemented over time.

These project success criteria will serve as essential benchmarks to assess the overall success of the COVID-19 Data Solution project at XYZ University, focusing on technical aspects, data quality, user experience, and the platform's impact on research and decision-making.



